{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClinicalCoding_Neural_Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7f0lgAw6e8_F",
        "Rnqy4DX6I8iB",
        "8uNYrgDkNFC8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkZfQJ_RlG8d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N1GUBv2-qu8"
      },
      "source": [
        "# **Setup** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qqh2fBh3JxkP"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from os import path\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyJPPLmRs4vH"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuXIAWn75FWj"
      },
      "source": [
        "%ls gdrive/MyDrive/CodiEsp/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fffqpHdoe1zd"
      },
      "source": [
        "# **D-subtask** *English*: **Data Loader**\n",
        "\n",
        "X_train, X_val, X_test: list of *input text data*\n",
        "\n",
        "Y_train, Y_val, Y_test: list of one-hot encoded *labels*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB01d6zVKLVh"
      },
      "source": [
        "df_train = pd.read_csv('drive/MyDrive/CodiEsp/train/trainD.tsv', sep='\\t', header=None)\n",
        "df_train.rename(columns={0:\"Id\", 1:\"ICD10\"}, inplace=True)\n",
        "print(\"Training Data:\")\n",
        "display(df_train.head())\n",
        "\n",
        "print(\"\\n\\nValidation Data:\")\n",
        "df_val = pd.read_csv('drive/MyDrive/CodiEsp/dev/devD.tsv', sep='\\t', header=None)\n",
        "df_val.rename(columns = {0:\"Id\", 1:\"ICD10\"}, inplace = True)\n",
        "display(df_val.head())\n",
        "\n",
        "print(\"\\n\\nTest Data:\")\n",
        "df_test = pd.read_csv('drive/MyDrive/CodiEsp/test/testD.tsv', sep='\\t', header=None)\n",
        "df_test.rename(columns={0:\"Id\", 1:\"ICD10\"}, inplace=True)\n",
        "display(df_test.head())\n",
        "\n",
        "df = pd.concat([df_train, df_val, df_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lyflba-KT-k"
      },
      "source": [
        "ids = df['Id'].unique()\n",
        "codes = df['ICD10'].unique()  \n",
        "\n",
        "print(\"Number of documents in training data:\", len(ids), \"\\nNumber of ICD10 codes:\", len(codes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ9m5j90K4qf"
      },
      "source": [
        "code2idx = {}\n",
        "for i in range(len(codes)):\n",
        "  code2idx[codes[i]] = i\n",
        "\n",
        "id2label = {}\n",
        "for i in range(len(ids)):\n",
        "  id2label[ids[i]] = [0]*len(codes)\n",
        "\n",
        "for i, data in df.iterrows():\n",
        "  _id = data[0]\n",
        "  _code = data[1]\n",
        "  id2label[_id][code2idx[_code]] = 1\n",
        "\n",
        "_id2label = [(id, y) for id, y in id2label.items()]\n",
        "ID, Y = zip(*_id2label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L8ih5HmzV62"
      },
      "source": [
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QT8dP-8yZBV"
      },
      "source": [
        "def remstopwords(text, stopwords):\n",
        "    text = re.sub('\\[\\*\\*[^\\]]*\\*\\*\\]', '', text)\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) \n",
        "    text = re.sub(\" \\d+\", \" \", text)\n",
        "    return \" \".join([i for i in text.split() if i not in stopwords])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62z-06hJRwEI"
      },
      "source": [
        "X_train = []\n",
        "Y_train = []\n",
        "\n",
        "for id in (df_train['Id'].unique()):\n",
        "  Y_train.append(id2label[id])\n",
        "\n",
        "  # with open('gdrive/MyDrive/CodiEsp/train/text_files_en/' + id + '.txt', 'r') as f:\n",
        "  #   text = f.read().replace('\\n', ' ')\n",
        "  # X_train.append(remstopwords(text.lower(), stop_words))\n",
        "# with open(\"gdrive/MyDrive/X_train.txt\", \"wb\") as fp:\n",
        "#   pickle.dump(X_train, fp)\n",
        "with open(\"drive/MyDrive/X_train.txt\", \"rb\") as fp:\n",
        "  X_train = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcNE2jvAPu5p"
      },
      "source": [
        "X_val = []\n",
        "Y_val = []\n",
        "\n",
        "for id in (df_val['Id'].unique()):\n",
        "  Y_val.append(id2label[id])\n",
        "\n",
        "  # with open('gdrive/MyDrive/CodiEsp/dev/text_files_en/' + id + '.txt', 'r') as f:\n",
        "  #   text = f.read().replace('\\n', ' ')\n",
        "  # X_val.append(remstopwords(text.lower(), stop_words))\n",
        "# with open(\"gdrive/MyDrive/X_val.txt\", \"wb\") as fp:\n",
        "#   pickle.dump(X_val, fp)\n",
        "with open(\"drive/MyDrive/X_val.txt\", \"rb\") as fp:\n",
        "  X_val = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1475dI1Rv_H"
      },
      "source": [
        "X_test = []\n",
        "Y_test = []\n",
        "\n",
        "for id in (df_test['Id'].unique()):\n",
        "  Y_test.append(id2label[id])\n",
        "\n",
        "#   with open('gdrive/MyDrive/CodiEsp/test/text_files_en/' + id + '.txt', 'r') as f:\n",
        "#     text = f.read().replace('\\n', ' ')\n",
        "#   X_test.append(remstopwords(text.lower(), stop_words))\n",
        "# with open(\"gdrive/MyDrive/X_test.txt\", \"wb\") as fp:\n",
        "#   pickle.dump(X_val, fp)\n",
        "with open(\"drive/MyDrive/X_test.txt\", \"rb\") as fp:\n",
        "  X_test = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uaJX6MvUUvh"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsQS3GQJTEiI"
      },
      "source": [
        "p_code = [0]*len(codes)\n",
        "for label in Y_train:\n",
        "  for i, code in enumerate(label):\n",
        "    if (code == 1):\n",
        "      p_code[i] = 1\n",
        "\n",
        "not_present = 0\n",
        "for i, present in enumerate(p_code):\n",
        "  if (present == 0):\n",
        "    not_present += 1\n",
        "\n",
        "print(\"Number of classes NOT PRESENT in training dataset:\", not_present)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMUxwlNddRC7"
      },
      "source": [
        "def hamming_score(y_true, y_pred, normalize = True, sample_weight = None):\n",
        "    ''' Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
        "        http://stackoverflow.com/q/32239577/395857 '''\n",
        "\n",
        "    acc_list = []\n",
        "    for i in range(y_true.shape[0]):\n",
        "        set_true = set(np.where(y_true[i])[0])\n",
        "        set_pred = set(np.where(y_pred[i])[0])\n",
        "        tmp_a = None\n",
        "        if len(set_true) == 0 and len(set_pred) == 0:\n",
        "            tmp_a = 1\n",
        "        else:\n",
        "            tmp_a = len(set_true.intersection(set_pred))/float(len(set_true.union(set_pred)))\n",
        "        acc_list.append(tmp_a)\n",
        "    return np.mean(acc_list)\n",
        "\n",
        "def print_score(y_pred, y_t, clf):\n",
        "    print(\"Clf: \", clf.__class__.__name__)\n",
        "    print(\"Hamming loss: {}\".format(hamming_loss(y_pred, y_t)))\n",
        "    print(\"Hamming score: {}\".format(hamming_score(y_pred, y_t)))\n",
        "    print(\"---\")    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0x1HPq8Ah-A"
      },
      "source": [
        "SOS = 0\n",
        "EOS = 1\n",
        "PAD = 2\n",
        "UNK = 3\n",
        "\n",
        "dictionary = {'START' : (1, SOS), 'END' : (1, EOS), 'PAD' : (1, PAD), 'UNK': (1, UNK)}\n",
        "\n",
        "max_length = -1\n",
        "index = 4\n",
        "for i in range(len(X_train)):\n",
        "  if i%50 == 0:\n",
        "    print(f'[INFO] At {i}th line, making the dictionary...')\n",
        "  tokenized = nltk.word_tokenize(X_train[i])\n",
        "  if len(tokenized) > max_length:\n",
        "    max_length = len(tokenized)\n",
        "  for t in tokenized:\n",
        "    if t not in dictionary.keys():\n",
        "      dictionary[t] = (1, index)\n",
        "      index += 1\n",
        "    else:\n",
        "      dictionary[t] = (dictionary[t][0]+1, dictionary[t][1])\n",
        "\n",
        "for i in range(len(X_val)):\n",
        "  if i%50 == 0:\n",
        "    print(f'[INFO] At {i}th line, making the dictionary...')\n",
        "  tokenized = nltk.word_tokenize(X_val[i])\n",
        "  if len(tokenized) > max_length:\n",
        "    max_length = len(tokenized)\n",
        "  for t in tokenized:\n",
        "    if t not in dictionary.keys():\n",
        "      dictionary[t] = (1, index)\n",
        "      index += 1\n",
        "    else:\n",
        "      dictionary[t] = (dictionary[t][0]+1, dictionary[t][1])\n",
        "\n",
        "max_length = max_length + 1\n",
        "print('-------------------------------------------------------')\n",
        "\n",
        "print(f'[INFO] Maximum length of the documents is : {max_length}')\n",
        "print(f'[INFO] Number of words in the dictionary : {len(dictionary)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZpwrf9CEbfh"
      },
      "source": [
        "X_train_indexed = []\n",
        "X_val_indexed = []\n",
        "X_test_indexed = []\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "  tokenized = nltk.word_tokenize(X_train[i])\n",
        "  tokenized = list(map(lambda x: dictionary[x][1], tokenized))\n",
        "  tokenized.append(dictionary['END'][1])\n",
        "  X_train_indexed.append(tokenized)\n",
        "\n",
        "for i in range(len(X_val)):\n",
        "  tokenized = nltk.word_tokenize(X_val[i])\n",
        "  tokenized = list(map(lambda x: dictionary[x][1], tokenized))\n",
        "  tokenized.append(dictionary['END'][1])\n",
        "  X_val_indexed.append(tokenized)\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "  tokenized = nltk.word_tokenize(X_test[i])\n",
        "  tokenized = [dictionary[x][1] if x in dictionary.keys() else dictionary['UNK'][1] for x  in tokenized]\n",
        "  tokenized.append(dictionary['END'][1])\n",
        "  X_test_indexed.append(tokenized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzMfIsJDDXgd"
      },
      "source": [
        "X_train_indexed = tf.keras.preprocessing.sequence.pad_sequences(X_train_indexed, maxlen=max_length, padding='post', value=dictionary['PAD'][1], dtype='float32')\n",
        "X_val_indexed = tf.keras.preprocessing.sequence.pad_sequences(X_val_indexed, maxlen=max_length, padding='post', value=dictionary['PAD'][1], dtype='float32')\n",
        "X_test_indexed = tf.keras.preprocessing.sequence.pad_sequences(X_test_indexed, maxlen=max_length, padding='post', value=dictionary['PAD'][1], dtype='float32')\n",
        "Y_train_indexed = np.array(Y_train)\n",
        "Y_val_indexed = np.array(Y_val)\n",
        "Y_test_indexed = np.array(Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76Rh9QJL5knK"
      },
      "source": [
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f0lgAw6e8_F"
      },
      "source": [
        "# **P-subtask** *English*: **Data Loader**\n",
        "\n",
        "X_train, X_val, X_test: list of *input text data*\n",
        "\n",
        "Y_train, Y_val, Y_test: list of one-hot encoded *labels*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoNq_bTZfB0c"
      },
      "source": [
        "df_train = pd.read_csv('drive/MyDrive/CodiEsp/train/trainP.tsv', sep='\\t', header=None)\n",
        "df_train.rename(columns={0:\"Id\", 1:\"ICD10\"}, inplace=True)\n",
        "print(\"Training Data:\")\n",
        "display(df_train.head())\n",
        "\n",
        "print(\"\\n\\nValidation Data:\")\n",
        "df_val = pd.read_csv('drive/MyDrive/CodiEsp/dev/devP.tsv', sep='\\t', header=None)\n",
        "df_val.rename(columns = {0:\"Id\", 1:\"ICD10\"}, inplace = True)\n",
        "display(df_val.head())\n",
        "\n",
        "print(\"\\n\\nTest Data:\")\n",
        "df_test = pd.read_csv('drive/MyDrive/CodiEsp/test/testP.tsv', sep='\\t', header=None)\n",
        "df_test.rename(columns={0:\"Id\", 1:\"ICD10\"}, inplace=True)\n",
        "display(df_test.head())\n",
        "\n",
        "df = pd.concat([df_train, df_val, df_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s1MHC9MfFF9"
      },
      "source": [
        "ids = df['Id'].unique()\n",
        "codes = df['ICD10'].unique()  \n",
        "\n",
        "print(\"Number of documents in training data:\", len(ids), \"\\nNumber of ICD10 codes:\", len(codes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FbI9Db1fIM0"
      },
      "source": [
        "code2idx = {}\n",
        "for i in range(len(codes)):\n",
        "  code2idx[codes[i]] = i\n",
        "\n",
        "id2label = {}\n",
        "for i in range(len(ids)):\n",
        "  id2label[ids[i]] = [0]*len(codes)\n",
        "\n",
        "for i, data in df.iterrows():\n",
        "  _id = data[0]\n",
        "  _code = data[1]\n",
        "  id2label[_id][code2idx[_code]] = 1\n",
        "\n",
        "_id2label = [(id, y) for id, y in id2label.items()]\n",
        "ID, Y = zip(*_id2label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKJMJKJffOJk"
      },
      "source": [
        "X_train = []\n",
        "Y_train = []\n",
        "\n",
        "for id in (df_train['Id'].unique()):\n",
        "  Y_train.append(id2label[id])\n",
        "\n",
        "  # with open('gdrive/MyDrive/CodiEsp/train/text_files_en/' + id + '.txt', 'r') as f:\n",
        "  #   text = f.read().replace('\\n', ' ')\n",
        "  # X_train.append(remstopwords(text.lower(), stop_words))\n",
        "# with open(\"gdrive/MyDrive/X_train.txt\", \"wb\") as fp:\n",
        "#   pickle.dump(X_train, fp)\n",
        "with open(\"drive/MyDrive/X_train_P.txt\", \"rb\") as fp:\n",
        "  X_train = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LQ3-gEdfUac"
      },
      "source": [
        "X_val = []\n",
        "Y_val = []\n",
        "\n",
        "for id in (df_val['Id'].unique()):\n",
        "  Y_val.append(id2label[id])\n",
        "\n",
        "  # with open('gdrive/MyDrive/CodiEsp/dev/text_files_en/' + id + '.txt', 'r') as f:\n",
        "  #   text = f.read().replace('\\n', ' ')\n",
        "  # X_val.append(remstopwords(text.lower(), stop_words))\n",
        "# with open(\"gdrive/MyDrive/X_val.txt\", \"wb\") as fp:\n",
        "#   pickle.dump(X_val, fp)\n",
        "with open(\"drive/MyDrive/X_val_P.txt\", \"rb\") as fp:\n",
        "  X_val = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpMDHOFhfU70"
      },
      "source": [
        "X_test = []\n",
        "Y_test = []\n",
        "\n",
        "for id in (df_test['Id'].unique()):\n",
        "  Y_test.append(id2label[id])\n",
        "\n",
        "#   with open('gdrive/MyDrive/CodiEsp/test/text_files_en/' + id + '.txt', 'r') as f:\n",
        "#     text = f.read().replace('\\n', ' ')\n",
        "#   X_test.append(remstopwords(text.lower(), stop_words))\n",
        "# with open(\"gdrive/MyDrive/X_test.txt\", \"wb\") as fp:\n",
        "#   pickle.dump(X_val, fp)\n",
        "with open(\"drive/MyDrive/X_test_P.txt\", \"rb\") as fp:\n",
        "  X_test = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfqLhYrohE8L"
      },
      "source": [
        "p_code = [0]*len(codes)\n",
        "for label in Y_train:\n",
        "  for i, code in enumerate(label):\n",
        "    if (code == 1):\n",
        "      p_code[i] = 1\n",
        "\n",
        "not_present = 0\n",
        "for i, present in enumerate(p_code):\n",
        "  if (present == 0):\n",
        "    not_present += 1\n",
        "\n",
        "print(\"Number of classes NOT PRESENT in training dataset:\", not_present)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGcthqlvhJka"
      },
      "source": [
        "SOS = 0\n",
        "EOS = 1\n",
        "PAD = 2\n",
        "UNK = 3\n",
        "\n",
        "dictionary = {'START' : (1, SOS), 'END' : (1, EOS), 'PAD' : (1, PAD), 'UNK': (1, UNK)}\n",
        "\n",
        "max_length = -1\n",
        "index = 4\n",
        "for i in range(len(X_train)):\n",
        "  if i%50 == 0:\n",
        "    print(f'[INFO] At {i}th line, making the dictionary...')\n",
        "  tokenized = nltk.word_tokenize(X_train[i])\n",
        "  if len(tokenized) > max_length:\n",
        "    max_length = len(tokenized)\n",
        "  for t in tokenized:\n",
        "    if t not in dictionary.keys():\n",
        "      dictionary[t] = (1, index)\n",
        "      index += 1\n",
        "    else:\n",
        "      dictionary[t] = (dictionary[t][0]+1, dictionary[t][1])\n",
        "\n",
        "for i in range(len(X_val)):\n",
        "  if i%50 == 0:\n",
        "    print(f'[INFO] At {i}th line, making the dictionary...')\n",
        "  tokenized = nltk.word_tokenize(X_val[i])\n",
        "  if len(tokenized) > max_length:\n",
        "    max_length = len(tokenized)\n",
        "  for t in tokenized:\n",
        "    if t not in dictionary.keys():\n",
        "      dictionary[t] = (1, index)\n",
        "      index += 1\n",
        "    else:\n",
        "      dictionary[t] = (dictionary[t][0]+1, dictionary[t][1])\n",
        "\n",
        "max_length = max_length + 1\n",
        "print('-------------------------------------------------------')\n",
        "\n",
        "print(f'[INFO] Maximum length of the documents is : {max_length}')\n",
        "print(f'[INFO] Number of words in the dictionary : {len(dictionary)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gToxkxnrhNoe"
      },
      "source": [
        "X_train_indexed = []\n",
        "X_val_indexed = []\n",
        "X_test_indexed = []\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "  tokenized = nltk.word_tokenize(X_train[i])\n",
        "  tokenized = list(map(lambda x: dictionary[x][1], tokenized))\n",
        "  tokenized.append(dictionary['END'][1])\n",
        "  X_train_indexed.append(tokenized)\n",
        "\n",
        "for i in range(len(X_val)):\n",
        "  tokenized = nltk.word_tokenize(X_val[i])\n",
        "  tokenized = list(map(lambda x: dictionary[x][1], tokenized))\n",
        "  tokenized.append(dictionary['END'][1])\n",
        "  X_val_indexed.append(tokenized)\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "  tokenized = nltk.word_tokenize(X_test[i])\n",
        "  tokenized = [dictionary[x][1] if x in dictionary.keys() else dictionary['UNK'][1] for x  in tokenized]\n",
        "  tokenized.append(dictionary['END'][1])\n",
        "  X_test_indexed.append(tokenized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo5MmDDJhOco"
      },
      "source": [
        "X_train_indexed = tf.keras.preprocessing.sequence.pad_sequences(X_train_indexed, maxlen=max_length, padding='post', value=dictionary['PAD'][1], dtype='float32')\n",
        "X_val_indexed = tf.keras.preprocessing.sequence.pad_sequences(X_val_indexed, maxlen=max_length, padding='post', value=dictionary['PAD'][1], dtype='float32')\n",
        "X_test_indexed = tf.keras.preprocessing.sequence.pad_sequences(X_test_indexed, maxlen=max_length, padding='post', value=dictionary['PAD'][1], dtype='float32')\n",
        "Y_train_indexed = np.array(Y_train)\n",
        "Y_val_indexed = np.array(Y_val)\n",
        "Y_test_indexed = np.array(Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AQQrHM0I5pN"
      },
      "source": [
        "# Model 1 Task D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1MlgwxM0tbK"
      },
      "source": [
        "model_1 = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(512, activation='relu', input_shape=(max_length,)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(Y_train_indexed.shape[1], activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyAGiKiL6j0s"
      },
      "source": [
        "model_1.compile(loss = 'binary_crossentropy' , optimizer = 'rmsprop' , metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EImTb_8BG133"
      },
      "source": [
        "model_1.fit(X_train_indexed, Y_train_indexed, epochs = 100, batch_size = 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llVNzbdW1dKg"
      },
      "source": [
        "predictions_train_1 = model_1.predict(X_train_indexed)\n",
        "predictions_rounded_train_1 = np.zeros(predictions_train_1.shape)\n",
        "\n",
        "for i in range(len(predictions_train_1)):\n",
        "  predictions_rounded_train_1[i] = np.where(predictions_train_1[i] < 0.2, 0, 1)\n",
        "\n",
        "hamming_score(Y_train_indexed, predictions_rounded_train_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNkCTZKr6-3N"
      },
      "source": [
        "predictions_val_1 = model_1.predict(X_val_indexed)\n",
        "predictions_rounded_val_1 = np.zeros(predictions_val_1.shape)\n",
        "\n",
        "for i in range(len(predictions_val_1)):\n",
        "  predictions_rounded_val_1[i] = np.where(predictions_val_1[i] < 0.2, 0, 1)\n",
        "\n",
        "hamming_score(Y_val_indexed, predictions_rounded_val_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNajbpEeNLT4"
      },
      "source": [
        "f1_score(Y_val_indexed, predictions_rounded_val_1, average='macro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxWN-3SidD0o"
      },
      "source": [
        "predictions_test_1 = model_1.predict(X_test_indexed)\n",
        "predictions_rounded_test_1 = np.zeros(predictions_test_1.shape)\n",
        "\n",
        "for i in range(len(predictions_test_1)):\n",
        "  predictions_rounded_test_1[i] = np.where(predictions_test_1[i] < 0.2, 0, 1)\n",
        "\n",
        "hamming_score(Y_test_indexed, predictions_rounded_test_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2fsdjzoMGTN"
      },
      "source": [
        "model_1.save('gdrive/MyDrive/model_1.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juIBtRf-hoED"
      },
      "source": [
        "# Model 1 Task P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfdBW0Jhhtly"
      },
      "source": [
        "model_1 = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(512, activation='relu', input_shape=(max_length,)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(Y_train_indexed.shape[1], activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N351a20qhvfR"
      },
      "source": [
        "model_1.compile(loss = 'binary_crossentropy' , optimizer = 'rmsprop' , metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz3hn9u3hwDx"
      },
      "source": [
        "model_1.fit(X_train_indexed, Y_train_indexed, epochs = 100, batch_size = 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8nLqZUThyiy"
      },
      "source": [
        "predictions_train_1 = model_1.predict(X_train_indexed)\n",
        "predictions_rounded_train_1 = np.zeros(predictions_train_1.shape)\n",
        "\n",
        "for i in range(len(predictions_train_1)):\n",
        "  predictions_rounded_train_1[i] = np.where(predictions_train_1[i] < 0.2, 0, 1)\n",
        "\n",
        "hamming_score(Y_train_indexed, predictions_rounded_train_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaEsrjmSh01i"
      },
      "source": [
        "predictions_val_1 = model_1.predict(X_val_indexed)\n",
        "predictions_rounded_val_1 = np.zeros(predictions_val_1.shape)\n",
        "\n",
        "for i in range(len(predictions_val_1)):\n",
        "  predictions_rounded_val_1[i] = np.where(predictions_val_1[i] < 0.2, 0, 1)\n",
        "\n",
        "hamming_score(Y_val_indexed, predictions_rounded_val_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NEga10Kh2k5"
      },
      "source": [
        "predictions_test_1 = model_1.predict(X_test_indexed)\n",
        "predictions_rounded_test_1 = np.zeros(predictions_test_1.shape)\n",
        "\n",
        "for i in range(len(predictions_test_1)):\n",
        "  predictions_rounded_test_1[i] = np.where(predictions_test_1[i] < 0.2, 0, 1)\n",
        "\n",
        "hamming_score(Y_test_indexed, predictions_rounded_test_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnqy4DX6I8iB"
      },
      "source": [
        "# Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB8SR9EL4B7v"
      },
      "source": [
        "model_2 = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape = (max_length,)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(Y_train_indexed.shape[1], activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A04bM-Wq46A3"
      },
      "source": [
        "model_2.compile(loss='binary_crossentropy' , optimizer='rmsprop' , metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7jqdwGb4_LB"
      },
      "source": [
        "model_2.fit(X_train_indexed, Y_train_indexed, epochs = 30, batch_size = 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5Pl3hJP5T_3"
      },
      "source": [
        "predictions_train_2 = model_2.predict(X_train_indexed)\n",
        "predictions_rounded_train_2 = np.zeros(predictions_train_2.shape)\n",
        "\n",
        "for i in range(len(predictions_train_2)):\n",
        "  predictions_rounded_train_2[i] = np.where(predictions_train_2[i] < 0.5, 0, 1)\n",
        "\n",
        "hamming_score(Y_train_indexed, predictions_rounded_train_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFeO57yHMhii"
      },
      "source": [
        "predictions_val_2 = model_2.predict(X_val_indexed)\n",
        "predictions_rounded_val_2 = np.zeros(predictions_val_2.shape)\n",
        "\n",
        "for i in range(len(predictions_val_2)):\n",
        "  predictions_rounded_val_2[i] = np.where(predictions_val_2[i] < 0.5, 0, 1)\n",
        "\n",
        "hamming_score(Y_val_indexed, predictions_rounded_val_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9P2jN5K5oGW"
      },
      "source": [
        "f1_score(Y_val_indexed, predictions_rounded_val_2, average='macro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2USYj5vVNP9W"
      },
      "source": [
        "model_2.save('drive/MyDrive/model_2.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uNYrgDkNFC8"
      },
      "source": [
        "# Model 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FcI4D-5zRyT"
      },
      "source": [
        "model_3 = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(len(dictionary), 1024, input_length = max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(Y_train_indexed.shape[1], activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGR1QccdOGqR"
      },
      "source": [
        "model_3.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSzZuISi974p"
      },
      "source": [
        "# early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=0, restore_best_weights=True)\n",
        "optimizer = tf.keras.optimizers.Adam(lr=0.002, clipnorm=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujztvUF77pg7"
      },
      "source": [
        "model_3.compile(loss='binary_crossentropy' , optimizer=optimizer , metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUTIllKn7tPQ"
      },
      "source": [
        "model_3.fit(X_train_indexed, Y_train_indexed, epochs=40, batch_size=64, validation_data=(X_val_indexed, Y_val_indexed,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgOhCn2K8E5S"
      },
      "source": [
        "predictions_val_3 = model_3.predict(X_val_indexed)\n",
        "predictions_rounded_val_3 = np.zeros(predictions_val_3.shape)\n",
        "\n",
        "for i in range(len(predictions_val_3)):\n",
        "  predictions_rounded_val_3[i] = np.where(predictions_val_3[i] < 0.2, 0, 1)\n",
        "\n",
        "hamming_score(Y_val_indexed, predictions_rounded_val_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdjZbiS_8Ept"
      },
      "source": [
        "f1_score(Y_val_indexed, predictions_rounded_val_3, average='macro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3s8dFkp_rmG"
      },
      "source": [
        "model_3.save('drive/MyDrive/model_3.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6uKAb3sZxy_"
      },
      "source": [
        "## Final Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxYzna9jZWeP"
      },
      "source": [
        "model_3 = tf.keras.models.load_model('drive/MyDrive/model_3.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWrospszZmgv"
      },
      "source": [
        "model_3.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcRfdRZWZsec"
      },
      "source": [
        "predictions_val_3 = model_3.predict(X_val_indexed)\n",
        "predictions_rounded_val_3 = np.zeros(predictions_val_3.shape)\n",
        "\n",
        "for i in range(len(predictions_val_3)):\n",
        "  predictions_rounded_val_3[i] = np.where(predictions_val_3[i] < 0.2, 0, 1)\n",
        "\n",
        "hamming_score(Y_val_indexed, predictions_rounded_val_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbrEvoLWZv5H"
      },
      "source": [
        "f1_score(Y_val_indexed, predictions_rounded_val_3, average='macro')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}